import time
import random
import re
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup

from config import *

class ConservativeLinkedInScraper:
    def __init__(self):
        self.driver = None
        self.jobs_data = []
        self.session_start_time = None
        
    def setup_driver(self):
        """å®‰å…¨è®¾ç½®æµè§ˆå™¨é©±åŠ¨"""
        print("ğŸš€ å¯åŠ¨æµè§ˆå™¨ï¼ˆå®‰å…¨æ¨¡å¼ï¼‰...")
        chrome_options = Options()
        
        # åæ£€æµ‹è®¾ç½®
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--no-sandbox")
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        self.driver.implicitly_wait(BROWSER_CONFIG["implicit_wait"])
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ - å®‰å…¨æ¨¡å¼æ¿€æ´»")
        
    def safe_delay(self, min_seconds=None, max_seconds=None):
        """å®‰å…¨å»¶è¿Ÿ"""
        if min_seconds is None:
            min_seconds = SAFETY_CONFIG["delay_between_jobs"][0]
        if max_seconds is None:
            max_seconds = SAFETY_CONFIG["delay_between_jobs"][1]
            
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
        
    def simulate_human_behavior(self):
        """æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # éšæœºé¼ æ ‡ç§»åŠ¨
        if random.random() > 0.7:
            try:
                actions = ActionChains(self.driver)
                x_offset = random.randint(-100, 100)
                y_offset = random.randint(-100, 100)
                actions.move_by_offset(x_offset, y_offset).perform()
                actions.move_by_offset(-x_offset, -y_offset).perform()
            except:
                pass
        
        # éšæœºæ»šåŠ¨
        if random.random() > 0.5:
            try:
                scroll_pixels = random.randint(200, 500)
                self.driver.execute_script(f"window.scrollBy(0, {scroll_pixels});")
            except:
                pass
        
        self.safe_delay(1, 3)
    
    def build_search_url(self):
        """æ„å»ºæœç´¢URL"""
        base_url = LINKEDIN_URL
        params = []
        
        for key, value in SEARCH_PARAMS.items():
            params.append(f"{key}={value}")
        
        search_url = f"{base_url}?{'&'.join(params)}"
        print(f"ğŸ” æœç´¢ç›®æ ‡: å¾·å›½äº”å¤§åŸå¸‚ | ç°åœº/æ··åˆåŠå…¬ | 24å°æ—¶å†…å‘å¸ƒ")
        return search_url
    
    def detect_english(self, text):
        """æ£€æµ‹è‹±æ–‡èŒä½æè¿°"""
        if not text:
            return 0.0
            
        text_lower = text.lower()
        matches = 0
        total_keywords = len(ENGLISH_DETECTION["required_keywords"])
        
        for keyword in ENGLISH_DETECTION["required_keywords"]:
            if re.search(r'\b' + re.escape(keyword) + r'\b', text_lower):
                matches += 1
        
        return matches / total_keywords
    
    def extract_work_arrangement(self, description, location_text):
        """æå–å·¥ä½œå®‰æ’ç±»å‹"""
        text = (description + " " + location_text).lower()
        
        if "hybrid" in text:
            return "hybrid"
        elif "on-site" in text or "on site" in text or "office" in text:
            return "on-site"
        elif "remote" in text or "work from home" in text:
            return "remote"
        else:
            return "unknown"
    
    def check_safety_limits(self, current_count):
        """æ£€æŸ¥å®‰å…¨é™åˆ¶"""
        if current_count >= SAFETY_CONFIG["max_jobs_per_session"]:
            print(f"ğŸ›‘ è¾¾åˆ°ä¼šè¯ä¸Šé™: {SAFETY_CONFIG['max_jobs_per_session']} ä¸ªèŒä½")
            return False
        
        # æ¯å¤„ç†10ä¸ªèŒä½ä¼‘æ¯ä¸€æ¬¡
        if current_count > 0 and current_count % SAFETY_CONFIG["session_break_after"] == 0:
            break_time = random.randint(SAFETY_CONFIG["break_duration"][0], SAFETY_CONFIG["break_duration"][1])
            print(f"â¸ï¸  å®‰å…¨æš‚åœ {break_time} ç§’...")
            time.sleep(break_time)
            
        return True
    
    def scrape_jobs(self):
        """å®‰å…¨çˆ¬å–èŒä½æ•°æ®"""
        try:
            # è®¿é—®æœç´¢é¡µé¢
            search_url = self.build_search_url()
            self.driver.get(search_url)
            
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
            self.safe_delay(5, 8)
            
            # ç­‰å¾…èŒä½åˆ—è¡¨
            WebDriverWait(self.driver, 25).until(
                EC.presence_of_element_located((By.CLASS_NAME, "jobs-search-results-list"))
            )
            
            job_count = 0
            processed_count = 0
            
            while self.check_safety_limits(processed_count):
                try:
                    # è·å–èŒä½åˆ—è¡¨
                    job_elements = self.driver.find_elements(
                        By.CSS_SELECTOR, "li.jobs-search-results__list-item"
                    )
                    
                    if job_count >= len(job_elements):
                        print("ğŸ“­ æ²¡æœ‰æ›´å¤šèŒä½äº†")
                        break
                    
                    print(f"\nğŸ“‹ å¤„ç†èŒä½ {processed_count + 1}/{SAFETY_CONFIG['max_jobs_per_session']}")
                    
                    # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                    self.simulate_human_behavior()
                    
                    # ç‚¹å‡»èŒä½
                    job_element = job_elements[job_count]
                    self.driver.execute_script("arguments[0].click();", job_element)
                    self.safe_delay(3, 5)
                    
                    # æå–èŒä½ä¿¡æ¯
                    job_data = self.extract_job_details()
                    if job_data:
                        # åˆ†æèŒä½
                        english_score = self.detect_english(job_data.get('description', ''))
                        is_english = english_score >= ENGLISH_DETECTION["min_english_score"]
                        
                        job_data['english_score'] = round(english_score, 2)
                        job_data['is_english'] = is_english
                        job_data['work_arrangement'] = self.extract_work_arrangement(
                            job_data.get('description', ''),
                            job_data.get('location', '')
                        )
                        
                        self.jobs_data.append(job_data)
                        
                        status = "âœ… è‹±æ–‡" if is_english else "âŒ éè‹±æ–‡"
                        work_type = job_data['work_arrangement']
                        print(f"   {job_data['title']}")
                        print(f"   {job_data['company']} | {job_data['location']}")
                        print(f"   ğŸ“Š è‹±æ–‡è¯„åˆ†: {english_score:.2f} | å·¥ä½œç±»å‹: {work_type} | {status}")
                        
                        processed_count += 1
                    
                    job_count += 1
                    
                    # æ»šåŠ¨åˆ°ä¸‹ä¸€ä¸ªèŒä½
                    if job_count < len(job_elements):
                        try:
                            next_job = job_elements[job_count]
                            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_job)
                            self.safe_delay(1, 2)
                        except:
                            pass
                    
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†èŒä½æ—¶å‡ºé”™: {e}")
                    job_count += 1
                    self.safe_delay(5, 8)  # å‡ºé”™æ—¶å»¶é•¿å»¶è¿Ÿ
                    continue
            
            print(f"\nğŸ‰ ä¼šè¯å®Œæˆ! å®‰å…¨å¤„ç† {len(self.jobs_data)} ä¸ªèŒä½")
            
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    def extract_job_details(self):
        """æå–èŒä½è¯¦æƒ…"""
        try:
            job_data = {}
            
            # æå–æ ‡é¢˜
            try:
                title_element = self.driver.find_element(
                    By.CSS_SELECTOR, 
                    ".job-details-jobs-unified-top-card__job-title, h2.job-details-jobs-unified-top-card__job-title"
                )
                job_data['title'] = title_element.text.strip()
            except:
                job_data['title'] = "Unknown Title"
            
            # æå–å…¬å¸
            try:
                company_element = self.driver.find_element(
                    By.CSS_SELECTOR, 
                    ".job-details-jobs-unified-top-card__company-name a, .job-details-jobs-unified-top-card__company-name"
                )
                job_data['company'] = company_element.text.strip()
            except:
                job_data['company'] = "Unknown Company"
            
            # æå–åœ°ç‚¹
            try:
                location_element = self.driver.find_element(
                    By.CSS_SELECTOR, 
                    ".job-details-jobs-unified-top-card__primary-description-container, .job-details-jobs-unified-top-card__bullet"
                )
                location_text = location_element.text
                if 'Â·' in location_text:
                    parts = location_text.split('Â·')
                    if len(parts) > 1:
                        job_data['location'] = parts[1].strip()
                    else:
                        job_data['location'] = location_text.strip()
                else:
                    job_data['location'] = location_text.strip()
            except:
                job_data['location'] = "Unknown Location"
            
            # æå–èŒä½æè¿°
            try:
                # å°è¯•ç‚¹å‡»"æ˜¾ç¤ºæ›´å¤š"
                try:
                    show_more_buttons = self.driver.find_elements(
                        By.CSS_SELECTOR, 
                        "button[aria-label='Show more']"
                    )
                    for button in show_more_buttons:
                        self.driver.execute_script("arguments[0].click();", button)
                        self.safe_delay(1, 2)
                except:
                    pass
                
                description_element = self.driver.find_element(
                    By.CSS_SELECTOR, 
                    "#job-details, .jobs-description, .jobs-description-content"
                )
                job_data['description'] = description_element.text.strip()
            except:
                job_data['description'] = ""
            
            # æå–èŒä½é“¾æ¥
            try:
                job_link_element = self.driver.find_element(
                    By.CSS_SELECTOR,
                    "a.jobs-search__job-details--container-embedded-link"
                )
                job_data['job_url'] = job_link_element.get_attribute('href')
            except:
                job_data['job_url'] = self.driver.current_url
            
            job_data['scraped_at'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return job_data
            
        except Exception as e:
            print(f"âš ï¸ æå–èŒä½è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            return None
    
    def save_to_excel(self):
        """ä¿å­˜ç»“æœåˆ°Excel"""
        if not self.jobs_data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        filename = f"german_jobs_{timestamp}.xlsx"
        
        df = pd.DataFrame(self.jobs_data)
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
        column_order = [
            'title', 'company', 'location', 'work_arrangement', 
            'is_english', 'english_score', 'job_url', 'scraped_at'
        ]
        
        existing_columns = [col for col in column_order if col in df.columns]
        other_columns = [col for col in df.columns if col not in column_order]
        
        df = df[existing_columns + other_columns]
        
        # åˆ›å»ºExcelæ–‡ä»¶
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # æ‰€æœ‰èŒä½
            df.to_excel(writer, sheet_name='All Jobs', index=False)
            
            # ä»…è‹±æ–‡èŒä½
            english_jobs = df[df['is_english'] == True]
            english_jobs.to_excel(writer, sheet_name='English Jobs', index=False)
            
            # æŒ‰å·¥ä½œç±»å‹åˆ†ç±»
            for work_type in ['hybrid', 'on-site']:
                type_jobs = english_jobs[english_jobs['work_arrangement'] == work_type]
                if not type_jobs.empty:
                    type_jobs.to_excel(writer, sheet_name=f'{work_type.title()} Jobs', index=False)
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“Š æœ¬æ¬¡ä¼šè¯ç»Ÿè®¡:")
        print(f"   æ€»èŒä½æ•°: {len(df)}")
        print(f"   è‹±æ–‡èŒä½: {len(english_jobs)}")
        
        if not english_jobs.empty:
            print("   å·¥ä½œç±»å‹åˆ†å¸ƒ:")
            work_stats = english_jobs['work_arrangement'].value_counts()
            for work_type, count in work_stats.items():
                print(f"     {work_type}: {count}")
    
    def close(self):
        """å®‰å…¨å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”š æµè§ˆå™¨å·²å®‰å…¨å…³é—­")

def main():
    print("=" * 50)
    print("ğŸ‡©ğŸ‡ª LinkedInå¾·å›½èŒä½ç­›é€‰å™¨ - ä¿å®ˆå®‰å…¨æ¨¡å¼")
    print("=" * 50)
    
    scraper = ConservativeLinkedInScraper()
    
    try:
        # è®¾ç½®æµè§ˆå™¨
        scraper.setup_driver()
        
        # å®‰å…¨æç¤º
        print("\nğŸ” å®‰å…¨æç¤º:")
        print("   â€¢ è¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•LinkedInè´¦å·")
        print("   â€¢ ç™»å½•åè„šæœ¬å°†è‡ªåŠ¨å¼€å§‹ï¼ˆå®‰å…¨å»¶è¿Ÿï¼‰")
        print("   â€¢ æœ¬æ¬¡ä¼šè¯æœ€å¤šå¤„ç†20ä¸ªèŒä½")
        print("   â€¢ æ¨èæ¯å¤©è¿è¡Œ2-3æ¬¡ï¼Œé—´éš”4å°æ—¶")
        input("\n   æŒ‰å›è½¦é”®ç»§ç»­...")
        
        # å¼€å§‹çˆ¬å–
        scraper.scrape_jobs()
        
        # ä¿å­˜ç»“æœ
        if scraper.jobs_data:
            scraper.save_to_excel()
            print(f"\nğŸ¯ ä¼šè¯å®Œæˆ!")
            print("   ä¸‹æ¬¡è¿è¡Œå»ºè®®åœ¨4å°æ—¶ä¹‹å")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°èŒä½æ•°æ®")
            
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        print("å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
    
    finally:
        scraper.close()

if __name__ == "__main__":
    main()
